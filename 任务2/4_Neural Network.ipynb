{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.rcParams[\"font.sans-serif\"]=[\"SimHei\"] #设置字体\n",
    "plt.rcParams[\"axes.unicode_minus\"]=False #该语句解决图像中的“-”负号的乱码问题\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BpNN(object):\n",
    "    def __init__(self, layer_dims_, learning_rate=0.1, seed=16, initializer='xavier', optimizer='gd'):\n",
    "\n",
    "        self.layer_dims_ = layer_dims_\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def fit(self, X_, y_, num_epochs=100):\n",
    "        '''\n",
    "        训练函数\n",
    "        参数：\n",
    "            X_:训练集特征\n",
    "            y_:训练集标签\n",
    "            num_epochs:迭代次数\n",
    "        返回：\n",
    "        '''\n",
    "        m, n = X_.shape\n",
    "        #print(X.shape)\n",
    "        layer_dims_ = copy.deepcopy(self.layer_dims_)\n",
    "        layer_dims_.insert(0, n)\n",
    "        #print(layer_dims_)\n",
    "\n",
    "        if y_.ndim == 1:\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "\n",
    "        self.parameters_ = xavier_initializer(layer_dims_, self.seed)\n",
    "\n",
    "        assert self.optimizer in ('gd', 'sgd')\n",
    "        if self.optimizer == 'gd':\n",
    "            parameters_, costs = self.optimizer_gd(X_, y_, self.parameters_, num_epochs, self.learning_rate)\n",
    "        elif self.optimizer == 'sgd':\n",
    "            parameters_, costs = self.optimizer_sgd(X_, y_, self.parameters_, num_epochs, self.learning_rate, self.seed)\n",
    "\n",
    "        self.parameters_ = parameters_\n",
    "        self.costs = costs\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_):\n",
    "\n",
    "        a_last, _ = self.forward_L_layer(X_, self.parameters_)\n",
    "        if a_last.shape[1] == 1:\n",
    "            predict_ = np.zeros(a_last.shape)\n",
    "            predict_[a_last>=0.5] = 1\n",
    "        else:\n",
    "            predict_ = np.argmax(a_last, axis=1)\n",
    "        return predict_\n",
    "\n",
    "    def compute_cost(self, y_hat_, y_):\n",
    "        if y_.ndim == 1:\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "        if y_.shape[1] == 1:\n",
    "            cost = cross_entry_sigmoid(y_hat_, y_)\n",
    "        else:\n",
    "            cost = cross_entry_softmax(y_hat_, y_)\n",
    "        return cost\n",
    "\n",
    "    def backward_one_layer(self, da_, cache_, activation_):\n",
    "        '''\n",
    "        反向传播1层\n",
    "        参数：\n",
    "            da_：当前层a梯度\n",
    "            cache_：参数缓存\n",
    "            activation_：激活函数\n",
    "        返回：\n",
    "            da_pre：上一层a的梯度\n",
    "            dw:w梯度\n",
    "            db:b梯度\n",
    "        '''\n",
    "        (a_pre_, w_, b_, z_) = cache_\n",
    "        m = da_.shape[0]\n",
    "\n",
    "        if activation_ == 'sigmoid':\n",
    "            dz_ = sigmoid_backward(da_, z_)\n",
    "        else:\n",
    "            dz_ = relu_backward(da_, z_)\n",
    "\n",
    "        dw = np.dot(dz_.T, a_pre_) / m                  #取样本均值\n",
    "        db = np.sum(dz_, axis=0, keepdims=True) / m\n",
    "        da_pre = np.dot(dz_, w_)\n",
    "\n",
    "        return da_pre, dw, db\n",
    "\n",
    "    def backward_L_layer(self, a_last, y_, caches):\n",
    "        '''\n",
    "        反向传播L层\n",
    "        参数：\n",
    "            a_last:\n",
    "            y_:\n",
    "            caches:参数缓存\n",
    "        返回：\n",
    "            grads:梯度矩阵\n",
    "        '''\n",
    "\n",
    "        grads = {}\n",
    "        L = len(caches)\n",
    "\n",
    "        if y_.ndim == 1:\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "\n",
    "        if y_.shape[1] == 1:  # 目标值只有一列表示为二分类 \n",
    "            da_last = -(y_ / a_last - (1 - y_) / (1 - a_last))          #链式求导得出，最后sigmoid层的da（即L对a的偏导）\n",
    "            da_pre_L_1, dwL_, dbL_ = self.backward_one_layer(da_last, caches[L - 1], 'sigmoid')\n",
    "\n",
    "\n",
    "        grads['da' + str(L)] = da_pre_L_1\n",
    "        grads['dW' + str(L)] = dwL_\n",
    "        grads['db' + str(L)] = dbL_\n",
    "\n",
    "        for i in range(L - 1, 0, -1):\n",
    "            da_pre_, dw, db = self.backward_one_layer(grads['da' + str(i + 1)], caches[i - 1], 'relu')\n",
    "\n",
    "            grads['da' + str(i)] = da_pre_\n",
    "            grads['dW' + str(i)] = dw\n",
    "            grads['db' + str(i)] = db\n",
    "        #print(grads)\n",
    "        return grads\n",
    "\n",
    "    def forward_one_layer(self, a_pre_, w_, b_, activation_):\n",
    "        '''\n",
    "        向前传播1层\n",
    "        参数:\n",
    "            a_pre_：上一层的函数值\n",
    "            w_:权重矩阵\n",
    "            b_：偏置项\n",
    "            acivation:激活函数\n",
    "        返回：\n",
    "            a_:该层函数值\n",
    "            cache_:缓存参数\n",
    "        '''\n",
    "        z_ = np.dot(a_pre_, w_.T) + b_\n",
    "        assert activation_ in ('sigmoid', 'relu', 'softmax')\n",
    "\n",
    "        if activation_ == 'sigmoid':\n",
    "            a_ = sigmoid(z_)\n",
    "        else:\n",
    "            a_ = relu(z_)\n",
    "\n",
    "        cache_ = (a_pre_, w_, b_, z_)  # 将向前传播过程中产生的数据保存下来，在向后传播过程计算梯度的时候要用上的。\n",
    "        return a_, cache_\n",
    "\n",
    "    def forward_L_layer(self, X_, parameters_):\n",
    "        '''\n",
    "        向前传播L层\n",
    "        参数：\n",
    "            X_：样本特征\n",
    "            parameters：权重参数\n",
    "        返回：\n",
    "            a_last:最后一层函数值\n",
    "            caches:所有缓存数据\n",
    "        '''\n",
    "        L_ = int(len(parameters_) / 2)                  #前进到最后一层\n",
    "        caches = []\n",
    "        a_ = X_                                         #第一层函数值就是样本数据\n",
    "        for i in range(1, L_):\n",
    "            w_ = parameters_['W' + str(i)]\n",
    "            b_ = parameters_['b' + str(i)]\n",
    "            a_pre_ = a_\n",
    "            a_, cache_ = self.forward_one_layer(a_pre_, w_, b_, 'relu')\n",
    "            caches.append(cache_)\n",
    "\n",
    "        w_last = parameters_['W' + str(L_)]\n",
    "        b_last = parameters_['b' + str(L_)]\n",
    "\n",
    "        if w_last.shape[0] == 1:\n",
    "            a_last, cache_ = self.forward_one_layer(a_, w_last, b_last, 'sigmoid')\n",
    "        else:\n",
    "            a_last, cache_ = self.forward_one_layer(a_, w_last, b_last, 'softmax')\n",
    "\n",
    "        caches.append(cache_)\n",
    "        return a_last, caches\n",
    "\n",
    "    def optimizer_gd(self, X_, y_, parameters_, num_epochs, learning_rate):\n",
    "        '''\n",
    "        标准bp算法\n",
    "        参数：\n",
    "            X_:样本特征\n",
    "            y_:样本标签\n",
    "            parameters_:权重矩阵\n",
    "            num_epochs:迭代次数\n",
    "            learning_rate:学习率\n",
    "        返回：\n",
    "            parameters_:权重矩阵\n",
    "            costs:代价值列表\n",
    "        '''\n",
    "        costs = []\n",
    "        for i in range(num_epochs):\n",
    "            a_last, caches = self.forward_L_layer(X_, parameters_)                          #使用所有样本计算参数和梯度\n",
    "            grads = self.backward_L_layer(a_last, y_, caches)\n",
    "\n",
    "            parameters_ = update_parameters_with_gd(parameters_, grads, learning_rate)\n",
    "            cost = self.compute_cost(a_last, y_)\n",
    "\n",
    "            costs.append(cost)\n",
    "\n",
    "        return parameters_, costs\n",
    "\n",
    "    def optimizer_sgd(self, X_, y_, parameters_, num_epochs, learning_rate, seed):\n",
    "        '''\n",
    "        sgd中，更新参数步骤和gd是一致的，只不过在计算梯度的时候是用一个样本而已。\n",
    "        '''\n",
    "        np.random.seed(seed)\n",
    "        costs = []\n",
    "        m_ = X_.shape[0]\n",
    "        for _ in range(num_epochs):\n",
    "            random_index = np.random.randint(0, m_)                                     #使用一个样本计算参数和梯度         \n",
    "            a_last, caches = self.forward_L_layer(X_[[random_index], :], parameters_)\n",
    "            grads = self.backward_L_layer(a_last, y_[[random_index], :], caches)\n",
    "\n",
    "            parameters_ = update_parameters_with_sgd(parameters_, grads, learning_rate)\n",
    "\n",
    "            a_last_cost, _ = self.forward_L_layer(X_, parameters_)\n",
    "\n",
    "            cost = self.compute_cost(a_last_cost, y_)\n",
    "\n",
    "            costs.append(cost)\n",
    "\n",
    "        return parameters_, costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "def cross_entry_softmax(y_hat_, y_):\n",
    "    '''\n",
    "    计算多分类时的交叉熵\n",
    "    :param y_hat_:\n",
    "    :param y_:\n",
    "    :return:\n",
    "    '''\n",
    "    m = y_.shape[0]\n",
    "    loss = -np.sum(y_ * np.log(y_hat_)) / m\n",
    "    return loss\n",
    "\n",
    "def xavier_initializer(layer_dims_, seed=16):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    parameters_ = {}\n",
    "    num_L = len(layer_dims_)\n",
    "    for l in range(num_L - 1):\n",
    "        temp_w = np.random.randn(layer_dims_[l + 1], layer_dims_[l]) * np.sqrt(1 / layer_dims_[l])\n",
    "        temp_b = np.zeros((1, layer_dims_[l + 1]))\n",
    "\n",
    "        parameters_['W' + str(l + 1)] = temp_w\n",
    "        parameters_['b' + str(l + 1)] = temp_b\n",
    "    #print(parameters_)\n",
    "    return parameters_\n",
    "\n",
    "\n",
    "def cross_entry_sigmoid(y_hat_, y_):\n",
    "    '''\n",
    "    计算在二分类时的交叉熵\n",
    "    :param y_hat_:  模型输出值\n",
    "    :param y_:      样本真实标签值\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    m = y_.shape[0]\n",
    "    loss = -(np.dot(y_.T, np.log(y_hat_)) + np.dot(1 - y_.T, np.log(1 - y_hat_))) / m\n",
    "\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(0, z)\n",
    "    return a\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)  # 防止过大，超出限制，导致计算结果为 nan\n",
    "    z_exp = np.exp(z)\n",
    "    softmax_z = z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "    return softmax_z\n",
    "\n",
    "\n",
    "def sigmoid_backward(da_, cache_z):\n",
    "    a = 1 / (1 + np.exp(-cache_z))\n",
    "    dz_ = da_ * a * (1 - a)\n",
    "    return dz_\n",
    "\n",
    "\n",
    "def softmax_backward(y_, cache_z):\n",
    "    #\n",
    "    a = softmax(cache_z)\n",
    "    dz_ = a - y_\n",
    "    assert dz_.shape == cache_z.shape\n",
    "    return dz_\n",
    "\n",
    "\n",
    "def relu_backward(da_, cache_z):\n",
    "    dz = np.array(da_, copy=True)\n",
    "    dz[cache_z <= 0] = 0\n",
    "    assert (dz.shape == cache_z.shape)\n",
    "\n",
    "    return dz\n",
    "\n",
    "\n",
    "def update_parameters_with_gd(parameters_, grads, learning_rate):\n",
    "    '''\n",
    "    标准bp参数更新\n",
    "    '''\n",
    "    L_ = int(len(parameters_) / 2)\n",
    "\n",
    "    for l in range(1, L_ + 1):\n",
    "        parameters_['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters_['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "\n",
    "    return parameters_\n",
    "\n",
    "\n",
    "def update_parameters_with_sgd(parameters_, grads, learning_rate):\n",
    "    '''\n",
    "    累积bp参数更新\n",
    "    '''\n",
    "    L_ = int(len(parameters_) / 2)\n",
    "\n",
    "    for l in range(1, L_ + 1):\n",
    "        parameters_['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters_['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "\n",
    "    return parameters_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.8095238095238095\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=pd.read_csv(\"X.csv\",index_col=0)\n",
    "y=pd.read_csv(\"y.csv\",index_col=0)\n",
    "y=y[\"status\"]\n",
    "m,n=X.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=1)\n",
    "bp1 = BpNN([20,10,1], learning_rate=0.01, optimizer='gd')\n",
    "bp1.fit(X_train.values, y_train.values, num_epochs=1000)\n",
    "plt.plot(bp1.costs)\n",
    "plt.title(\"标准BP代价函数值\")\n",
    "y_pre=bp1.predict(X_test)\n",
    "y_pre=[b for a in y_pre for b in a ]\n",
    "acc=0\n",
    "for i in range(0,len(y_pre)):\n",
    "    if y_pre[i]==y_test.iloc[i]:\n",
    "        acc+=1\n",
    "print('准确率：', acc/len(X_test))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
