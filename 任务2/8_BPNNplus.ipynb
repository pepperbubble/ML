{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "plt.rcParams[\"font.sans-serif\"]=[\"SimHei\"] #设置字体\n",
    "plt.rcParams[\"axes.unicode_minus\"]=False #该语句解决图像中的“-”负号的乱码问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BpNN(object):\n",
    "    def __init__(self, layer_dims_, learning_rate=0.1, seed=16, initializer='he', optimizer='gd'):\n",
    "\n",
    "        self.layer_dims_ = layer_dims_\n",
    "        self.learning_rate = learning_rate\n",
    "        self.seed = seed\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def fit(self, X_, y_, num_epochs=100):\n",
    "        m, n = X_.shape\n",
    "        layer_dims_ = copy.deepcopy(self.layer_dims_)\n",
    "        layer_dims_.insert(0, n)\n",
    "\n",
    "        if y_.ndim == 1:\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "\n",
    "        assert self.initializer in ('he', 'xavier')\n",
    "\n",
    "        if self.initializer == 'he':\n",
    "            self.parameters_ = xavier_initializer(layer_dims_, self.seed)\n",
    "        elif self.initializer == 'xavier':\n",
    "            self.parameters_ = xavier_initializer(layer_dims_, self.seed)\n",
    "\n",
    "        assert self.optimizer in ('gd', 'sgd', 'adam', 'momentum')\n",
    "        if self.optimizer == 'gd':\n",
    "            parameters_, costs = self.optimizer_gd(X_, y_, self.parameters_, num_epochs, self.learning_rate)\n",
    "        elif self.optimizer == 'sgd':\n",
    "            parameters_, costs = self.optimizer_sgd(X_, y_, self.parameters_, num_epochs, self.learning_rate, self.seed)\n",
    "        elif self.optimizer == 'momentum':\n",
    "            parameters_, costs = self.optimizer_sgd_monment(X_, y_, self.parameters_, beta=0.9, num_epochs=num_epochs,\n",
    "                                                            learning_rate=self.learning_rate, seed=self.seed)\n",
    "        elif self.optimizer == 'adam':\n",
    "            parameters_, costs = self.optimizer_sgd_adam(X_, y_, self.parameters_, beta1=0.9, beta2=0.999, epsilon=1e-7,\n",
    "                                                         num_epochs=num_epochs, learning_rate=self.learning_rate,\n",
    "                                                         seed=self.seed)\n",
    "\n",
    "        self.parameters_ = parameters_\n",
    "        self.costs = costs\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X_):\n",
    "        if not hasattr(self, \"parameters_\"):\n",
    "            raise Exception('you have to fit first before predict.')\n",
    "\n",
    "        a_last, _ = self.forward_L_layer(X_, self.parameters_)\n",
    "        if a_last.shape[1] == 1:\n",
    "            predict_ = np.zeros(a_last.shape)\n",
    "            predict_[a_last>=0.5] = 1\n",
    "        else:\n",
    "            predict_ = np.argmax(a_last, axis=1)\n",
    "        return predict_\n",
    "\n",
    "    def compute_cost(self, y_hat_, y_):\n",
    "        if y_.ndim == 1:\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "        if y_.shape[1] == 1:\n",
    "            cost = cross_entry_sigmoid(y_hat_, y_)\n",
    "        else:\n",
    "            cost = cross_entry_softmax(y_hat_, y_)\n",
    "        return cost\n",
    "\n",
    "    def backward_one_layer(self, da_, cache_, activation_):\n",
    "        # 在activation_ 为'softmax'时， da_实际上输入是y_， 并不是\n",
    "        (a_pre_, w_, b_, z_) = cache_\n",
    "        m = da_.shape[0]\n",
    "\n",
    "        assert activation_ in ('sigmoid', 'relu', 'softmax')\n",
    "\n",
    "        if activation_ == 'sigmoid':\n",
    "            dz_ = sigmoid_backward(da_, z_)\n",
    "        elif activation_ == 'relu':\n",
    "            dz_ = relu_backward(da_, z_)\n",
    "        else:\n",
    "            dz_ = softmax_backward(da_, z_)\n",
    "\n",
    "        dw = np.dot(dz_.T, a_pre_) / m\n",
    "        db = np.sum(dz_, axis=0, keepdims=True) / m\n",
    "        da_pre = np.dot(dz_, w_)\n",
    "\n",
    "        assert dw.shape == w_.shape\n",
    "        assert db.shape == b_.shape\n",
    "        assert da_pre.shape == a_pre_.shape\n",
    "\n",
    "        return da_pre, dw, db\n",
    "\n",
    "    def backward_L_layer(self, a_last, y_, caches):\n",
    "\n",
    "        grads = {}\n",
    "        L = len(caches)\n",
    "\n",
    "        if y_.ndim == 1:\n",
    "            y_ = y_.reshape(-1, 1)\n",
    "\n",
    "        if y_.shape[1] == 1:  # 目标值只有一列表示为二分类\n",
    "            da_last = -(y_ / a_last - (1 - y_) / (1 - a_last))\n",
    "            da_pre_L_1, dwL_, dbL_ = self.backward_one_layer(da_last, caches[L - 1], 'sigmoid')\n",
    "\n",
    "        else:  # 经过one hot，表示为多分类\n",
    "\n",
    "            # 在计算softmax的梯度时，可以直接用 dz = a - y可计算出交叉熵损失函数对z的偏导， 所以这里第一个参数输入直接为y_\n",
    "            da_pre_L_1, dwL_, dbL_ = self.backward_one_layer(y_, caches[L - 1], 'softmax')\n",
    "\n",
    "        grads['da' + str(L)] = da_pre_L_1\n",
    "        grads['dW' + str(L)] = dwL_\n",
    "        grads['db' + str(L)] = dbL_\n",
    "\n",
    "        for i in range(L - 1, 0, -1):\n",
    "            da_pre_, dw, db = self.backward_one_layer(grads['da' + str(i + 1)], caches[i - 1], 'relu')\n",
    "\n",
    "            grads['da' + str(i)] = da_pre_\n",
    "            grads['dW' + str(i)] = dw\n",
    "            grads['db' + str(i)] = db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def forward_one_layer(self, a_pre_, w_, b_, activation_):\n",
    "        z_ = np.dot(a_pre_, w_.T) + b_\n",
    "        assert activation_ in ('sigmoid', 'relu', 'softmax')\n",
    "\n",
    "        if activation_ == 'sigmoid':\n",
    "            a_ = sigmoid(z_)\n",
    "        elif activation_ == 'relu':\n",
    "            a_ = relu(z_)\n",
    "        else:\n",
    "            a_ = softmax(z_)\n",
    "\n",
    "        cache_ = (a_pre_, w_, b_, z_)  # 将向前传播过程中产生的数据保存下来，在向后传播过程计算梯度的时候要用上的。\n",
    "        return a_, cache_\n",
    "\n",
    "    def forward_L_layer(self, X_, parameters_):\n",
    "        L_ = int(len(parameters_) / 2)\n",
    "        caches = []\n",
    "        a_ = X_\n",
    "        for i in range(1, L_):\n",
    "            w_ = parameters_['W' + str(i)]\n",
    "            b_ = parameters_['b' + str(i)]\n",
    "            a_pre_ = a_\n",
    "            a_, cache_ = self.forward_one_layer(a_pre_, w_, b_, 'relu')\n",
    "            caches.append(cache_)\n",
    "\n",
    "        w_last = parameters_['W' + str(L_)]\n",
    "        b_last = parameters_['b' + str(L_)]\n",
    "\n",
    "        if w_last.shape[0] == 1:\n",
    "            a_last, cache_ = self.forward_one_layer(a_, w_last, b_last, 'sigmoid')\n",
    "        else:\n",
    "            a_last, cache_ = self.forward_one_layer(a_, w_last, b_last, 'softmax')\n",
    "\n",
    "        caches.append(cache_)\n",
    "        return a_last, caches\n",
    "\n",
    "    def optimizer_gd(self, X_, y_, parameters_, num_epochs, learning_rate):\n",
    "        costs = []\n",
    "        for i in range(num_epochs):\n",
    "            a_last, caches = self.forward_L_layer(X_, parameters_)\n",
    "            grads = self.backward_L_layer(a_last, y_, caches)\n",
    "\n",
    "            parameters_ = update_parameters_with_gd(parameters_, grads, learning_rate)\n",
    "            cost = self.compute_cost(a_last, y_)\n",
    "\n",
    "            costs.append(cost)\n",
    "\n",
    "        return parameters_, costs\n",
    "\n",
    "    def optimizer_sgd(self, X_, y_, parameters_, num_epochs, learning_rate, seed):\n",
    "        '''\n",
    "        sgd中，更新参数步骤和gd是一致的，只不过在计算梯度的时候是用一个样本而已。\n",
    "        '''\n",
    "        np.random.seed(seed)\n",
    "        costs = []\n",
    "        m_ = X_.shape[0]\n",
    "        for _ in range(num_epochs):\n",
    "            random_index = np.random.randint(0, m_)\n",
    "\n",
    "            a_last, caches = self.forward_L_layer(X_[[random_index], :], parameters_)\n",
    "            grads = self.backward_L_layer(a_last, y_[[random_index], :], caches)\n",
    "\n",
    "            parameters_ = update_parameters_with_sgd(parameters_, grads, learning_rate)\n",
    "\n",
    "            a_last_cost, _ = self.forward_L_layer(X_, parameters_)\n",
    "\n",
    "            cost = self.compute_cost(a_last_cost, y_)\n",
    "\n",
    "            costs.append(cost)\n",
    "\n",
    "        return parameters_, costs\n",
    "\n",
    "    def optimizer_sgd_monment(self, X_, y_, parameters_, beta, num_epochs, learning_rate, seed):\n",
    "        '''\n",
    "\n",
    "        :param X_:\n",
    "        :param y_:\n",
    "        :param parameters_: 初始化的参数\n",
    "        :param v_:          梯度的指数加权移动平均数\n",
    "        :param beta:        冲量大小，\n",
    "        :param num_epochs:\n",
    "        :param learning_rate:\n",
    "        :param seed:\n",
    "        :return:\n",
    "        '''\n",
    "        np.random.seed(seed)\n",
    "        costs = []\n",
    "        m_ = X_.shape[0]\n",
    "        velcoity = initialize_velcoity(parameters_)\n",
    "        for _ in range(num_epochs):\n",
    "            random_index = np.random.randint(0, m_)\n",
    "\n",
    "            a_last, caches = self.forward_L_layer(X_[[random_index], :], parameters_)\n",
    "            grads = self.backward_L_layer(a_last, y_[[random_index], :], caches)\n",
    "\n",
    "            parameters_, v_ = update_parameters_with_sgd_momentum(parameters_, grads, velcoity, beta,\n",
    "                                                                           learning_rate)\n",
    "            a_last_cost, _ = self.forward_L_layer(X_, parameters_)\n",
    "            cost = self.compute_cost(a_last_cost, y_)\n",
    "            costs.append(cost)\n",
    "\n",
    "        return parameters_, costs\n",
    "\n",
    "    def optimizer_sgd_adam(self, X_, y_, parameters_, beta1, beta2, epsilon, num_epochs, learning_rate, seed):\n",
    "        '''\n",
    "\n",
    "        :param X_:\n",
    "        :param y_:\n",
    "        :param parameters_: 初始化的参数\n",
    "        :param v_:          梯度的指数加权移动平均数\n",
    "        :param beta:        冲量大小，\n",
    "        :param num_epochs:\n",
    "        :param learning_rate:\n",
    "        :param seed:\n",
    "        :return:\n",
    "        '''\n",
    "        np.random.seed(seed)\n",
    "        costs = []\n",
    "        m_ = X_.shape[0]\n",
    "        velcoity, square_grad = initialize_adam(parameters_)\n",
    "        for epoch in range(num_epochs):\n",
    "            random_index = np.random.randint(0, m_)\n",
    "\n",
    "            a_last, caches = self.forward_L_layer(X_[[random_index], :], parameters_)\n",
    "            grads = self.backward_L_layer(a_last, y_[[random_index], :], caches)\n",
    "\n",
    "            parameters_, velcoity, square_grad = update_parameters_with_sgd_adam(parameters_, grads, velcoity,\n",
    "                                                                                          square_grad, epoch + 1,\n",
    "                                                                                          learning_rate, beta1, beta2,\n",
    "                                                                                          epsilon)\n",
    "            a_last_cost, _ = self.forward_L_layer(X_, parameters_)\n",
    "            cost = self.compute_cost(a_last_cost, y_)\n",
    "            costs.append(cost)\n",
    "\n",
    "        return parameters_, costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def xavier_initializer(layer_dims_, seed=16):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    parameters_ = {}\n",
    "    num_L = len(layer_dims_)\n",
    "    for l in range(num_L - 1):\n",
    "        temp_w = np.random.randn(layer_dims_[l + 1], layer_dims_[l]) * np.sqrt(1 / layer_dims_[l])\n",
    "        temp_b = np.zeros((1, layer_dims_[l + 1]))\n",
    "\n",
    "        parameters_['W' + str(l + 1)] = temp_w\n",
    "        parameters_['b' + str(l + 1)] = temp_b\n",
    "\n",
    "    return parameters_\n",
    "\n",
    "\n",
    "def he_initializer(layer_dims_, seed=16):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    parameters_ = {}\n",
    "    num_L = len(layer_dims_)\n",
    "    for l in range(num_L - 1):\n",
    "        temp_w = np.random.randn(layer_dims_[l + 1], layer_dims_[l]) * np.sqrt(2 / layer_dims_[l])\n",
    "        temp_b = np.zeros((1, layer_dims_[l + 1]))\n",
    "\n",
    "        parameters_['W' + str(l + 1)] = temp_w\n",
    "        parameters_['b' + str(l + 1)] = temp_b\n",
    "\n",
    "    return parameters_\n",
    "\n",
    "\n",
    "def cross_entry_sigmoid(y_hat_, y_):\n",
    "    '''\n",
    "    计算在二分类时的交叉熵\n",
    "    :param y_hat_:  模型输出值\n",
    "    :param y_:      样本真实标签值\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    m = y_.shape[0]\n",
    "    loss = -(np.dot(y_.T, np.log(y_hat_)) + np.dot(1 - y_.T, np.log(1 - y_hat_))) / m\n",
    "\n",
    "    return np.squeeze(loss)\n",
    "\n",
    "\n",
    "def cross_entry_softmax(y_hat_, y_):\n",
    "    '''\n",
    "    计算多分类时的交叉熵\n",
    "    :param y_hat_:\n",
    "    :param y_:\n",
    "    :return:\n",
    "    '''\n",
    "    m = y_.shape[0]\n",
    "    loss = -np.sum(y_ * np.log(y_hat_)) / m\n",
    "    return loss\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    a = 1 / (1 + np.exp(-z))\n",
    "    return a\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    a = np.maximum(0, z)\n",
    "    return a\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    z -= np.max(z)  # 防止过大，超出限制，导致计算结果为 nan\n",
    "    z_exp = np.exp(z)\n",
    "    softmax_z = z_exp / np.sum(z_exp, axis=1, keepdims=True)\n",
    "    return softmax_z\n",
    "\n",
    "\n",
    "def sigmoid_backward(da_, cache_z):\n",
    "    a = 1 / (1 + np.exp(-cache_z))\n",
    "    dz_ = da_ * a * (1 - a)\n",
    "    assert dz_.shape == cache_z.shape\n",
    "    return dz_\n",
    "\n",
    "\n",
    "def softmax_backward(y_, cache_z):\n",
    "    #\n",
    "    a = softmax(cache_z)\n",
    "    dz_ = a - y_\n",
    "    assert dz_.shape == cache_z.shape\n",
    "    return dz_\n",
    "\n",
    "\n",
    "def relu_backward(da_, cache_z):\n",
    "    dz = np.array(da_, copy=True)\n",
    "    dz[cache_z <= 0] = 0\n",
    "    assert (dz.shape == cache_z.shape)\n",
    "\n",
    "    return dz\n",
    "\n",
    "\n",
    "def update_parameters_with_gd(parameters_, grads, learning_rate):\n",
    "    L_ = int(len(parameters_) / 2)\n",
    "\n",
    "    for l in range(1, L_ + 1):\n",
    "        parameters_['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters_['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "\n",
    "    return parameters_\n",
    "\n",
    "\n",
    "def update_parameters_with_sgd(parameters_, grads, learning_rate):\n",
    "    L_ = int(len(parameters_) / 2)\n",
    "\n",
    "    for l in range(1, L_ + 1):\n",
    "        parameters_['W' + str(l)] -= learning_rate * grads['dW' + str(l)]\n",
    "        parameters_['b' + str(l)] -= learning_rate * grads['db' + str(l)]\n",
    "\n",
    "    return parameters_\n",
    "\n",
    "\n",
    "def initialize_velcoity(paramters):\n",
    "    v = {}\n",
    "\n",
    "    L_ = int(len(paramters) / 2)\n",
    "\n",
    "    for l in range(1, L_ + 1):\n",
    "        v['dW' + str(l)] = np.zeros(paramters['W' + str(l)].shape)\n",
    "        v['db' + str(l)] = np.zeros(paramters['b' + str(l)].shape)\n",
    "\n",
    "    return v\n",
    "\n",
    "\n",
    "def update_parameters_with_sgd_momentum(parameters, grads, velcoity, beta, learning_rate):\n",
    "    L_ = int(len(parameters) / 2)\n",
    "\n",
    "    for l in range(1, L_ + 1):\n",
    "        velcoity['dW' + str(l)] = beta * velcoity['dW' + str(l)] + (1 - beta) * grads['dW' + str(l)]\n",
    "        velcoity['db' + str(l)] = beta * velcoity['db' + str(l)] + (1 - beta) * grads['db' + str(l)]\n",
    "\n",
    "        parameters['W' + str(l)] -= learning_rate * velcoity['dW' + str(l)]\n",
    "        parameters['b' + str(l)] -= learning_rate * velcoity['db' + str(l)]\n",
    "\n",
    "    return parameters, velcoity\n",
    "\n",
    "\n",
    "def initialize_adam(paramters_):\n",
    "    l = int(len(paramters_) / 2)\n",
    "    square_grad = {}\n",
    "    velcoity = {}\n",
    "    for i in range(l):\n",
    "\n",
    "        for i in range(l):\n",
    "            square_grad['dW' + str(i + 1)] = np.zeros(paramters_['W' + str(i + 1)].shape)\n",
    "            square_grad['db' + str(i + 1)] = np.zeros(paramters_['b' + str(i + 1)].shape)\n",
    "            velcoity['dW' + str(i + 1)] = np.zeros(paramters_['W' + str(i + 1)].shape)\n",
    "            velcoity['db' + str(i + 1)] = np.zeros(paramters_['b' + str(i + 1)].shape)\n",
    "        return velcoity, square_grad\n",
    "\n",
    "\n",
    "def update_parameters_with_sgd_adam(parameters_, grads_, velcoity, square_grad, epoch, learning_rate=0.1, beta1=0.9,\n",
    "                                    beta2=0.999, epsilon=1e-8):\n",
    "    l = int(len(parameters_) / 2)\n",
    "\n",
    "    for i in range(l):\n",
    "        velcoity['dW' + str(i + 1)] = beta1 * velcoity['dW' + str(i + 1)] + (1 - beta1) * grads_['dW' + str(i + 1)]\n",
    "        velcoity['db' + str(i + 1)] = beta1 * velcoity['db' + str(i + 1)] + (1 - beta1) * grads_['db' + str(i + 1)]\n",
    "\n",
    "        vw_correct = velcoity['dW' + str(i + 1)] / (1 - np.power(beta1, epoch))         # 这里是对迭代初期的梯度进行修正\n",
    "        vb_correct = velcoity['db' + str(i + 1)] / (1 - np.power(beta1, epoch))\n",
    "\n",
    "        square_grad['dW' + str(i + 1)] = beta2 * square_grad['dW' + str(i + 1)] + (1 - beta2) * (\n",
    "                    grads_['dW' + str(i + 1)] ** 2)\n",
    "        square_grad['db' + str(i + 1)] = beta2 * square_grad['db' + str(i + 1)] + (1 - beta2) * (\n",
    "                    grads_['db' + str(i + 1)] ** 2)\n",
    "\n",
    "        sw_correct = square_grad['dW' + str(i + 1)] / (1 - np.power(beta2, epoch))\n",
    "        sb_correct = square_grad['db' + str(i + 1)] / (1 - np.power(beta2, epoch))\n",
    "\n",
    "        parameters_['W' + str(i + 1)] -= learning_rate * vw_correct / np.sqrt(sw_correct + epsilon)\n",
    "        parameters_['b' + str(i + 1)] -= learning_rate * vb_correct / np.sqrt(sb_correct + epsilon)\n",
    "\n",
    "    return parameters_, velcoity, square_grad\n",
    "\n",
    "\n",
    "def set_ax_gray(ax):\n",
    "    ax.patch.set_facecolor(\"gray\")\n",
    "    ax.patch.set_alpha(0.1)\n",
    "    ax.spines['right'].set_color('none')  # 设置隐藏坐标轴\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.spines['bottom'].set_color('none')\n",
    "    ax.spines['left'].set_color('none')\n",
    "    ax.grid(axis='y', linestyle='-.')\n",
    "\n",
    "\n",
    "def plot_costs(costs, labels, colors=None):\n",
    "    if colors is None:\n",
    "        colors = ['C', 'lightcoral']\n",
    "\n",
    "    ax = plt.subplot()\n",
    "    assert len(costs) == len(labels)\n",
    "    for i in range(len(costs)):\n",
    "        ax.plot(costs[i], color=colors[i], label=labels[i])\n",
    "    set_ax_gray(ax)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_xlabel('num epochs')\n",
    "    ax.set_ylabel('cost')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率： 0.7619047619047619\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEFCAYAAADDkQ0WAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAjLUlEQVR4nO3deXxU9b3/8ddnJgvZEyBsIRBBAVFANNjCdcEFbxdtrW2vWqv1amv1Wrva1l7b3j7s/XW1dtFaS6vVXr2V1rVarHUDQRAJIlL2NUggJGzZ15nv7485WWZByJjcnIT38/HII5OTMzOf7xDe853v+Z7vMeccIiIycAT6uwAREekZBbeIyACj4BYRGWAU3CIiA4yCW0RkgFFwi6+YWaqZPWFmOWaWbWb3etuHJdg3z8xeNrM07+dCM3vEzFKO8NjpZmbHWMcXzGyI9/jTzOzrZpZrZveZ2TlHuI+ZWXa3nwNmlnUszyfSEwpu8ZvZwBjnXB3QAlznbf8fM7spZt9Lgb3OuVbv5y8CHwTeF/ugZpYPrALKzCz2q8HMnJmd0e0uKcC3gXYgC7jCOVcLXADsPkLtc4HV3cJ6HFBhZqleDenH9AqIHEXCnolIP/p34HcAzrk2M2vzetBfBW6O2fcWImGNmZ0CfBg4A3jczP7NObe1Y0fn3GHg1I6fzWwMcDVwFvAs8JBzbof3uyHAHsABBUQC+VWvp53tnNvu7ZfunGvpVs+XgV8Aj5jZWCADSAWWex39VDOb1e2NRiQp6nGLb5hZEXAlkZ5xh4NAPtAIvGZm13n7Xk4kpPeY2XjgUeBGL3w/BzxvZp+IefwcM3vce56LgPFAxz413XZNA84BrgJKgamAATcBAa+XXgWsN7Mc77H/FfgIsAD4DHCnd7vcu30nMEehLb1BwS1+cieRgMw0syvM7AdALvAq8EvgRGC711v+MbAJOAl4BfgpsNd7nP3A14H/MrN5AGb2EDALuBf4LdAMVAFPec95jpk96N2/CdgOVABLgMPAIWAicI9zrhR4mcjwSZ2ZjSbyKaGWyNDKKOAbwD+IDPXs9H7O660XSo5vGioRXzCzjxMZE34JCAGnAcuBKcAC59yCbvvOJBLyFwNbgXne7auAHxEZ5x4HzHDOhb27pQJh59wiM1sGfNTb/mXn3CYzu5Cu/w9jiYT/N4A7gP8iMp6eSqSXj/f427zbk4EH6Oq9h4iE/J3dmjgRCCPSC9TjFr94DvgkkXBrcc7d5px7GvgbcH73HZ1zq51z93T96LYRGU/e421rB1q7hTZExqsxswnecxlQCDxrZid59+kwFPgKkR7zHOAZ4Brv9mjvQGeOc+6gV8Ai59z3YtqzD3i429e+nr4gIkei4BZfcM41Ouf2JPjVM8Al3gHDdzOMyNDHEZ+CyLj1QuA/vW3VwG3Ai0BRt1pWETloeRiYBPwE2OScawKeAJ4mMlTybpqIfBro+Go6yv4ix0zBLb7mnKsiMnxyK4CZndtxQDDGXOCf7/JQBqwFPuD9/GngkHPucSLhXRuz/xgiB0n/TmRMvOOA6RNE3gCeOUrpJcD3un2VHGV/kWOm4Ba/SQGCMdtuBa41s68B99M1rS+VyHkvXwUqnHMd86vN++rOiAT1TiK97yeB+wCcc38C6mL230pkVkg6cDtwipld5m27FZhvZrHzxY3IjBQDVjvnzur4AlYTmZGi40rynumPSPwmlUj4dXLO7TOzC4BfAzuA17vtOxG4Arik213Sva/uKoBfxJw4+YVuP2cTCVfMLJPI2PpbwGXOuUozuwr4DyIzSTab2Srg+2Z2qXOusdvzfga4DAiZ2dKYGh4DfgP8zzG8DiJHZLqQgojIwKKhEhGRAUbBLSIywCi4RUQGmD4/ODl8+HBXUlLS108jIjKorFq1ar9zrjDR7/o8uEtKSigrK+vrpxERGVTMrPxIv9NQiYjIAKPgFhEZYBTcIiIDjIJbRGSAUXCLiAwwCm4RkQFGwS0iMsD4NrhX7jzIXf/YRGu7rvYkItKdb4P7zfJD/OrlrbSHFdwiIt35NrgD3jrJYa06KyISxbfB3bG+fVjrhYuIRPFxcEeSW7ktIhLNt8Ed8HrcukKPiEg03wZ3x5UANcYtIhLNt8EdCHQMlSi5RUS6821wm2aViIgk5N/g9r6rxy0iEs23wd0xj1uxLSIS7ZiC28xGmtkS7/Y4M1tkZi+b2XzrGNPo7cI0j1tEJKGjBreZFQAPAVneps8DNznnzgeKgWl9UVjXCTh98egiIgPXsfS4Q8DlQC2Ac+5259wG73fDgP2xdzCzG8yszMzKqqurkyqs6wQcJbeISHdHDW7nXK1zriZ2u5ldDqxzzu1JcJ/5zrlS51xpYWHCq8sfvTCdOSkiklBKMncyswnArcCFvVtOt+fwvmuMW0QkWo9nlXhj3n8CrkvUE+8tAa8y5baISLRkpgPeBowD7vZml5zbyzUB3Zd1VXKLiHR3zEMlzrm53vdvAt/sq4JiaVaJiEg035+Ao1NwRESi+T641eMWEYnm2+DWFXBERBLzbXB3XUihf+sQEfEb3wa3aVaJiEhC/g1u77tyW0Qkmm+DW6e8i4gk5t/g9irTUImISDTfBrehMW4RkUT8G9wds0r6twwREd/xbXAHtB63iEhCvg1uXQFHRCQx3wa3ZpWIiCTm2+DWKe8iIon5N7g1q0REJCHfBndAq7qKiCTk3+AOaFlXEZFEfBvculiwiEhi/g3ujlkl/VyHiIjf+Da4A5pVIiKSkG+D23TmpIhIQr4Nbl0BR0QkMR8Ht2aViIgk4tvg7qAxbhGRaMcU3GY20syWeLdTzexZM1tmZtf1WWFaq0REJKGjBreZFQAPAVnepluAMufcHOBiM8vpk8K8ynRwUkQk2rH0uEPA5UCt9/Nc4M/e7WVAaewdzOwGMyszs7Lq6uqkCutaqySpu4uIDFpHDW7nXK1zrqbbpiygwrtdC4xMcJ/5zrlS51xpYWFhcoV1XgFHyS0i0l0yByfrgQzvdnaSj3FUplklIiIJJRO6q4CzvNszgJ29Vk03ndec1Bi3iEiUlCTu8xCw0MzOBqYCK3q3pAjNKhERSeyYe9zOubne93JgHvAacKFzLtQnhWmtEhGRhJLpceOc20PXzJI+oVklIiKJ+fbMSY1xi4gk5tvg7rgCjnJbRCSab4NbV8AREUnMt8Ed0BVwREQS8nFwR76rxy0iEs23wU1ncPdvGSIifuPb4A6YLoEjIpKI74NbPW4RkWi+DW7NKhERScy3wa21SkREEvNtcJtXmXrcIiLR/Bvc3nfltohINN8Gd9cJOEpuEZHufB/cmlUiIhLNt8FtOnNSRCQh3we3cltEJJpvg7trOqCSW0SkO98Gd9cJOP1ahoiI7/g2uHUCjohIYr4Nbh2cFBFJzMfBrTFuEZFEfBvcELmYgmJbRCRaj4PbzArMbKGZLTGz+/qiqA4BMw2ViIjESKbHfTXwsHPubCDHzEp7uaZOZppVIiISK5ngPgBMNrN8oBjY1asVdWNmmlUiIhIjmeBeCpwEfBHYCByK3cHMbjCzMjMrq66uTr4408FJEZFYyQT3D4AbnXN3EAnuf4/dwTk33zlX6pwrLSwsTLo4Q2PcIiKxkgnuTGCamQWB99GHEz8iPe6+enQRkYEpmeD+ITAfqAGGAn/q1Yq6icwq6atHFxEZmFJ6egfn3BvAKX1QSzzTmZMiIrF8fgKOHX0nEZHjjM+DWz1uEZFYvg5u05mTIiJxfB3cmlUiIhLP18FtmlUiIhLH38GNzpwUEYnl6+AOaK0SEZE4Pg9uzSoREYnl6+DWGLeISDyfBzc4XQNHRCSKr4NbY9wiIvF8HdymMW4RkTi+Dm6tDigiEs/XwW26Ao6ISBxfB7eu8i4iEs/XwR00Ixzu7ypERPzF18Gtg5MiIvF8HdwaKhERiefr4A4GNKtERCSWr4Nba5WIiMTzdXBrrRIRkXi+Du6AQVjJLSISxdfBHRnjVnCLiHTn6+DWxYJFROIlHdxmdq+ZXdKbxcSKHJzsy2cQERl4kgpuMzsbGOWce6aX64kSMNMYt4hIjB4Ht5mlAr8DdprZR3u/pC4a4xYRiZdMj/saYD3wE+BMM7sldgczu8HMysysrLq6OuniNB1QRCReMsE9E5jvnKsEHgbOi93BOTffOVfqnCstLCxMvjidgCMiEieZ4N4KTPBulwLlvVdOtKBmlYiIxElJ4j73Aw+Y2RVAKvCJ3i2pi2lZVxGROD0ObudcHfDJPqgljoZKRETi+foEHC3rKiISz9fBrWVdRUTi+Tq4dQUcEZF4vg5unTkpIhLP58GttUpERGL5O7h1yruISBx/B7cZym0RkWg+D24IaaxERCSKr4NbqwOKiMTzdXBrdUARkXi+Du6AgVOPW0Qkis+D2wgpuEVEovg+uHUCjohINN8HtzrcIiLRfB7caKhERCSGr4Nb0wFFROL5Org1HVBEJJ6vg1vTAUVE4vk8uE2nvIuIxPB3cOsKOCIicfwd3Bb5ruESEZEuPg/uSHJruEREpIvPgzvyXbktItLF38HtJbfmcouIdEk6uM1spJmt7s1iYnUMlSi3RUS6vJce951ARm8VkkjHUIlOexcR6ZJUcJvZ+UADUNm75UTr6HFrqEREpEuPg9vM0oDvAre9yz43mFmZmZVVV1cnX1zHUEk46YcQERl0kulx3wb82jl3+Eg7OOfmO+dKnXOlhYWFyRfXOatEPW4RkQ7JBPeFwM1mtgg4zcx+37sldemYVaIxbhGRLik9vYNz7pyO22a2yDn32d4tqYvGuEVE4r2nedzOubm9VEdCmg4oIhLP3yfgdEwH1KmTIiKd/B3cOnNSRCSOv4NbQyUiInF8HtyR7+pxi4h08Xlwa1lXEZFY/g7uzjHufi5ERMRH/B3cugKOiEgcnwe3etwiIrF8HtyR7xrjFhHp4vPg1jxuEZFYAyK4ldsiIl38HdxedVodUESki7+DW0MlIiJxBkRwazqgiEiXARHcmlQiItLF58Ed+a7pgCIiXfwd3FrWVUQkjr+Du2OoRFd5FxHp5OvgDupiwSIicXwd3Ckdwa0ut4hIJ18Hd0ePuz2kHreISAdfB3dKUBdSEBGJ5e/g7uhxK7hFRDr5OriD3mIl6nGLiHRJ6ekdzCwPeNS7bz1wuXOutbcLAwiaetwiIrGS6XFfBdzlnJsHVAIf6N2SugSDmlUiIhKrxz1u59y93X4sBKp6r5xoXdMB++oZREQGnqTHuM1sNlDgnHs9we9uMLMyMyurrq5Ourig5nGLiMRJKrjNbChwN3Bdot875+Y750qdc6WFhYVJF6dZJSIi8Xoc3GaWBvwZ+JZzrrz3S+rS1eNWcIuIdEimx309cAZwu5ktMrPLe7mmTinedED1uEVEuiRzcPI3wG/6oJY46nGLiMTz+Qk4WqtERCSWr4O76wo4mlUiItLB18FtZqQErHM97rv+sYnHVu3u56pERPpXj8e4/68FA0Z72LGpso5fvbwVgIunj2ZIarCfKxMR6R++7nFDZC53KOT429t7Oret3HmwHysSEelf/g/uYIC2UJgXN1QxrSiP1KDx2tYD/V2WiEi/8X1wD0kNcKChlY2VtZw3uZApo3L5Z0VNf5clItJvfB/cGalB3thxkLCDmeMLOLUol7UVNThdQFhEjlO+D+4hqUGq6loAmFmcz6lFedQ0tbH7UFM/VyYi0j98H9wZaZHZIxOGZ5Gfmca0ojwADZeIyHHL98E9JCUS3OOGZQIwaWQOKQFjrYJbRI5Tvg/ujh53YXY6EBk6mTQyR8HdS3SsQGTg8X1wp6dESizMSe/cNqukgFXlh2g7Di6N89zavXz50dXUNbf16uP+fsl2Zv/wJa554A0ONvTJJUNFpI/4PrgbWkMAFBVkdG57/4RhNLaGeHv3kXvdmyrruO7Blfzvil19XmNvamhpp6q2GYj0hm97Yi1PvbWHr//l7V57jrZQmJ88v4m9Nc0s2bKfLy94q9ceW0T6nu+De0zeEKBrqARg9sRhDEkN8OgbXaFcXdfSufzrok1VXPrr13h5YxX/+eRaNlXWHdNzPbNmD7c/uZbW9r7pyS/btp8r5i/n4ruX8MiK8rhhimXb9nPhXYs5785FrNh+gL01zdQ0tZGVFuTv6yqpONw1k2ZrVR2PrCinPYlPHVur6mltD/PLK07jxnMnsnRLNYfU6z4u1be088OFG7jn5S39XYr0gO/XKvnWB09mdF4G500Z0bktPzONT505ngde20GLF7J/XbOHcycVct1ZJ/C5P5bRFnL8+OPT+M7T63j49XK+f+mp7/o8P31+I79+ZRsA44dlcsM5E3ul/l0HGhmRm86Q1CB3PLOejd6byO1P/pOSYVmcOiaP+1/bwYa9tbywfh8Fmak0tIa45U+rO2v+9sVT+dYTa3nyzd184fyTqGls46rfr2BfbQs1TW38x9wTe1RTx4ycU8bkMW5oJvct3sbSrfu5ZMaYXmlzb2sPhXllUzVr3jnM3MmFlJYM7e+SBo0fLNzQ+an0nEmFTB+bH/X7rVX1NLeFONWbzTVQ1TS18cn7lvHR04q4+bye/X/xI98Hd15mKl+68KS47d/84GTM4C9l79AWcswqKWDx5moWb65m/LBMnr75X8jPTGPF9oMsWPkOYecImBEMGEX5GeRlpPLs2r1U1TZz8fTR/Hbxdj48bTSHGlu564XN7D7UxGnF+ZwyJo/quhbW761haFY6l80sYv3eWla/c5iPnjaG3CGpR6z9gaU7uOPZ9ZQMy+RXV85kY2Ud3/jAZKYX5fPp+1fw+Ju7+Z/l5fx9XSXDs9O45fwTuWnuRF7dvJ8bH17F/Fe3A5FFtV5cv4+fv7iFC6eO5H9X7KK6roWRuenc8/JWPjaziNF5GUesAyLDLpv31TMqdwjr9tSSkRrkhOFZAORnprJ4czVFBRl8ZcFbTBmVw91Xnk5aSvwHsnDYMX/JdjZV1vHtD5/MMO+TUGNrO0+8WcHUMbmcPq7gmP99j8Y5xzcee5snVlcA8IfXdrDg87OPOUhCYUd7OEx6St8sSvbi+n3sq2vmU2eOw8z65DmOZsPeWtbtqeXjpxf1qIZw2PHC+n2cO6mQ1bsO8dvF2/n1Vad3/n5rVR0X3vUqAKu/M4+CrLRer/3/yqubq9m8r56fPr9Jwd2f0lOCfOfiqXz7wyfjHAQCxt//Wcm26nquPHMc+ZmRP7JvfGAKe2qaeGbNHgLeglV1Le0ADM9OY0hqkDv/sZkROen896WnEnKOr/15DY+s2MUfl8dfUvO7T/+TRm/c/UcLNzBuWBbOOUZ7Qzp7a5pxDmYU5/HEmxWMyElnX20LH7nnNQDmThrB1DG5XF5azIKydwC47YNTuPHcrh7++VNGkDMkhVXlhzhxRDY5Q1K585MzuOCuxVw5/3UON7XxmdklXH/WCcz7+WIu/+3rzJs6kuXbDjBxRDZfOO9ESoZnkp4SpK65jXV7avnJ3zfy5q7D5A5JoT3smFGc13mhirNPKuTptypYumU/lbXNlB9o5MFlO7ho6iiefmsPJcMzuWT6GOpa2vl/f1vPn8siS+sebGjlgWtn8dc1Ffz4uU1U1jaTnhLg8Zvm9KiH5pzrDJz99S28c7CR04rzMTMeXLaTJ1ZXcNPciVwxq5gr57/O9Q+t5Kmb/4XReRm8vv0AhxpaOXdyIZlp0X/O9S3tfOI3y6iua2HB59/PiSNyAPjTG7t49I1dfOfiqZSWDKXicBPZaSnkZR75TTiR/fUtfPaPZUBkKO+iU0b16P4QeWPp+HdIpC0UZumW/Uwbm0dmWpDmtjAGFGSl4Zzj8TcruPUvawDYuLeWb1889Zife/3eWqrrWrjkA2OYMjqH3726nd2HGhlbkMmhhlau/cPKzn2ffXsPV88uAaDicBN1zW1MGZV7zM/12Krd3Pn8Jm6aO5Hh2els3lfH5FE5fGja6GN+jPfijR1dC9PVNre9a4drILC+ng5WWlrqysrK+vQ5eqqqrpnDjW1MLMymLRTmhfX7OGN8AWPyu3qt4bBj+fYDHGhoZWROOsVDM1m6dT+vbKxi+th8JhRmsWhTNRWHm2huDdHY1k5Ta4icIam0hcLs3N/A7InD+dm/zWBbdT3XPbiSf5k4nHs+NRMzY/2eWj5272sUD81k4RfPjuvd/mDhBua/up1bL5rEF86PfOJ4cvVuvrJgDUX5GSz80tnkZaSydMt+PvvHlTS3hTllTC7bqutpbosMHwUD1jnuPzw7jWvnlPCbRdtoaA1FvVk8tbqi8wDlM184i1+8uJmXNlZF1RMMGM45wg5uOGcCYwsy+O7T6xiWlcaBhlamj83jP+aeyPefXU9NUxvTx+bxzqFG2todYwsyyEgLUl3XQtg5RuVlUJCZSsCMsHMs2lRNXkYqp4/L58UNVdS3tHP2ScMZlTuEv6zazbypI7nv02cQDBgb9tbyyfuWM7Ygg+lj8zrfRE4Zk8s3PzCFCYVZpKcEaW4Lccez63lh/T4AivIz+Oq8SWSlB7nlT6tpCznyMlK5YMoInnqrgsy0FB64dhZm0NIW5sQR2Yzy3ozDYcfrOw5QtvMQ508ZwalFeZQfaODrf3mbN7yVKscNzeT5L5/TOX011qubq/nH+koKMiOdhY+fPpaHlu/kvsXbSE8J8LmzJ/C1iyYDUNPYRs6QFAIB49a/rIlbgz4zLcgfrp3FPa9sZcmW/Zx5wlBy0lN4aWMVj904m9KSoTjnaGgNkZ0eeTNrC4V5Y8dBpo7O7ew53/3SFn72wmZW3n4hbaEwZ/34ZT79/vHc8dFT+cqCt3hmzR4eu2kOtz3+Nm2hMM996RwaWto572eLONzYxsPXv485E4fx4LKdFA/NZN7UkQnbvudwExfetbizw9PdjLF5/OrKmYwflpXwvrEaWtpJTwmQEuzZ4bkP/XIJ6/fWAvDIZ9/HE29W0BoK87NPziAUdvzixc185LQxnDKmZ0NC4bAj8C5vvO+Fma1yzpUm/N3xGNz9oT0Ujvtj21/fQu6Q1CMOSWyorOXkUblRfxibKusYlTskqne4taoeM5hYmM2uA42s2HGAqroWGlrayc9MZUx+BudNHkFWegqvbz/A8+sq+dpFkzv/U4fCjl++uJkZxflccPJIDtS3cM8rWynMSeeymWMpKz/I2ooa0lOCXDR1JKcW5eGc4+6Xt/LGjoNcOrOIy2YWEQgYW6vq+fmLm6msaWZU3hAyUoPsPtRIU1uY4VlpBANGZW3koGvYOcLhyKeT1vYwr28/SGlJAdOL8vj90h04B5fPKuZbH5oSNdTxyqYqbvB6ule/v4QThmfyw+c2JgyG710ylenF+XzzsbfZUlUPwJRROfzo49O545l1rH7nMHMnFbKtuoFdBxuj7jtuaCbpKQGq61s43Ng1HXPSyGzKDzSSFgzwvY+cwuj8IXzqdyuYUJhFfkYqLe1h2kOOtnCYUNjR0hamsraZFG9teYhc3SnsYFpRHmkpAVaVH2LyyBwONLSwv76VoVlpGHCgoZXLTi+iKD+DJq99C1a+Q11LOykB4z8/dDLXzB5PQ2uIC+9aTHVdC1NH51LX0sY7B5soHprB0Mw0Kg43s7++hSGpAaaPzWfSyGxeWL+P4oJMHrtpDhD5NPnH5eXMmzqSF9bv44vnn8hXL5rMSxv2cf1DZVzgHWfqeFMvHprBzOIC/romsuTy9WedwCubqqhtivTGPzaziMmjcrjtibfZVtXAP75yDm2hMM1tYYqHZvDH5eX89PlNFOVncPmsYtJSAnzijLH85O8b2VhZx+WzipkxNp+n36rg0plFLNpUzS9f3MLYggy+f+mpnD6uAIejLeRYvm0/lTXNXDJjDMOy0zsP/O/Y38Cq8kN88/G3uWZ2CQ8u28mskgJW7jwEwLVzSmhuC/HoynfISU9h4ZfOZkRuOpU1zQxJDWJEpiJ3fCLcfaiR+5fuoKq2hY2VtVTVtnD/tbN4fl0lwYBx6WlFTByR1StDcwpuGXDaQ2HMOyaRSHVdC2YwvNsYe9nOQ+w53ERbKEwwEOD08fmdH+edcyzeXM3hxjYuOmVk57BKxxtqVV0zv128nUkjsxmTn8Gmyjre3HWIcDgyLPH+CUN53wnDeOqtCl7bup/ReUP46rzJnb3y+xZv45WNVaQGA6SlBEgLBggGI1dwSgkEmFCYxfVnnUB72LF2dw2PrCjnjPEFXDunhLCDX720hTW7DzM8O53igkx2HWwkJWBMG5vHlWeOi3odFm2q4r//toGvzpsUNdSwt6aJB5ft5I0dBxmenU7JsEyq6lo42NDqLReRy+5DTfyzooZNlXUMSQ1y/7WzOK04H4j0yr/06GoWrq1k7uRC5l9d2tmp+MNrO/j+s+sxM27/0MlMHZPLp3+/gvaw4zOzx/O3tZXsr2+hZFgmU8fksm5PLeUHIm+EOekp/OrKmVETDDosWLmL7/11PU1t0W+6I3MjQ4yxzj5pONurG6JmWHWXGjSy0lOoa24nIzVIvTcsmp2ewsIvns3nH17Fhr21TB6Zw4zirk9s86aO5I0dB2nw9m/vdoHy4dlppKcEaQuFOdTYSlvIMX5YJhOGZ7F5X31cLWnBALkZKaSnBPn6v07m0plFCWs9GgW3iETpGEKLfWN0zrG/vpXh2WlxBzrLDzSQEgxQ5A0pbquup7apjZnjCqisaWZbdT1zJg7DzAiHHX9ds4eG1nYunjbmXY8fOBfpNa+tOMxzayuZPCqHT5wxlte3H2TJlmrmTh7B2ooaJhRmcd7kETS3hXh+XSV7DjcTMDCDKaNyGZGbzpNvVtDYGiIrPYWGlnamjM5hxth8igsyyctMZVX5QZ5bW8l1Z53A0Kw0/vDaTsLOccM5Eyg/0MiClbtIDQYYPyyTUDjSIVi/txbDSA0aeZmpXDO7pPM1KD/QwANLd3DBySPJz0zl7d01bK9uoKktREt7iE+cPpY5Jw5P6t9IwS0iMsC8W3D7/gQcERGJllRwm9n9ZrbMzL7d2wWJiMi763Fwm9llQNA5NwcYY2bxZ8eIiEifSabHPRf4s3f7ZeCs2B3M7AYzKzOzsurq6vdQnoiIxEomuLOACu92LRA36945N985V+qcKy0sLHwv9YmISIxkgrse6DjFMDvJxxARkSQlE7qr6BoemQHs7LVqRETkqJJZZOopYImZjQE+CLy/VysSEZF3ldQJOGZWAMwDXnXOVR5l32ogfpm9YzMc2J/kfQcqtfn4oDYfH95Lm8c75xIeJOzzMyffCzMrO9KZQ4OV2nx8UJuPD33VZh1YFBEZYBTcIiIDjN+De35/F9AP1Objg9p8fOiTNvt6jFtEROL5vcctIiIxFNwiIgOMb4N7MC8da2Z5Zvacmb1gZk+aWVqi9g7G18DMRprZau/28dLme83sEu/2oG6zmRWY2UIzW2Jm93nbBm2bvb/nJd7tVDN71mvXdT3Z1lO+DO7jYOnYq4C7nHPzgErgCmLaO4hfgzuBjETtG4xtNrOzgVHOuWeOkzZfDTzsnDsbyDGzbzBI2+ydiPgQkYX3AG4Byrx2XWxmOT3Y1iO+DG6OYenYgcw5d69z7gXvx0Lg08S3d26CbQOamZ0PNBB5s5rLIG+zmaUCvwN2mtlHOQ7aDBwAJptZPlAMlDB42xwCLieySipEt2sZUNqDbT3i1+A+6tKxg4GZzQYKgHeIb++geg3MLA34LnCbtylR+wZVm4FrgPXAT4AzgZsZ/G1eCpwEfBHYCKQzSNvsnKt1ztV023Ssf9Pvuf1+De5Bv3SsmQ0F7gauI3F7B9trcBvwa+fcYe/n46HNM4H53no+DwOvMvjb/APgRufcHUSC+1MM/jZ3ONa/6ffcfr++YIN66Viv9/ln4FvOuXISt3ewvQYXAjeb2SLgNOASBn+btwITvNulRIYNBnubM4FpZhYE3gf8iMHf5g7H+v/4Pbc/mWVd/y88xeBeOvZ64AzgdjO7HfgDcHVMex2D6DVwzp3TcdsL748Q375B1WbgfuABM7sCSCUytvnXQd7mHxL5ex4PLAd+zuD/d+7wELDQOyA9FVhBZEjkWLb1iG/PnOzJ0rGDQaL2DvbXQG1WmxlkbfbejM4Cnu8Y/z7WbT16Hr8Gt4iIJObXMW4RETkCBbeIyACj4BYRGWAU3CIiA4yCW0RkgPn/S3/VG5dvGjwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X=pd.read_csv(\"X.csv\",index_col=0)\n",
    "y=pd.read_csv(\"y.csv\",index_col=0)\n",
    "y=y[\"status\"]\n",
    "m,n=X.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,random_state=1)\n",
    "bp1 = BpNN([200,10,1], learning_rate=0.001, optimizer='adam')\n",
    "bp1.fit(X_train.values, y_train.values, num_epochs=1000)\n",
    "plt.plot(bp1.costs)\n",
    "plt.title(\"代价函数值\")\n",
    "y_pre=bp1.predict(X_test)\n",
    "y_pre=[b for a in y_pre for b in a ]\n",
    "acc=0\n",
    "for i in range(0,len(y_pre)):\n",
    "    if y_pre[i]==y_test.iloc[i]:\n",
    "        acc+=1\n",
    "print('准确率：', acc/len(X_test))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91bb753b057673435fb8d6f6a083e6c818364728098c7ae050ca3a25357dd754"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
